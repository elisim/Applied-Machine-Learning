{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Todo:\n",
    " 1. check the order of the preprocess. what to do first === DONE ===\n",
    " 2. create pipeline \n",
    " 3. hyper param search\n",
    " 4. after findind hyper-params, add fit on all data (include val) \n",
    " 5. remove noisy samples: one option - remove rows with num of nan > trash\n",
    " 6. feature selection (annova, pca) \n",
    " 7. check for cat boost \n",
    " 8. check SMOTETomek and undersample & SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from copy import copy\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_test():\n",
    "    TRAIN_PATH = 'input/saftey_efficay_myopiaTrain.csv'\n",
    "    TEST_PATH = 'input/saftey_efficay_myopiaTest.csv'\n",
    "    train = pd.read_csv(TRAIN_PATH, low_memory=False)\n",
    "    test = pd.read_csv(TEST_PATH, low_memory=False)\n",
    "    return train,test\n",
    "\n",
    "def remove_columns(train, test, columns_to_remove):\n",
    "    train = train.drop(columns=columns_to_remove)\n",
    "    test = test.drop(columns=columns_to_remove)\n",
    "    return train, test\n",
    "\n",
    "def preprocess_data(train, test, pipeline):\n",
    "    \"\"\"\n",
    "    pipleline - pipeline to fit_transform the concat dummies before spliting to train & test\n",
    "    return: train & train dummies after pipeline fit_transform\n",
    "    \"\"\"\n",
    "    n_train = train.shape[0]\n",
    "    concat = pd.concat(objs=[train, test], axis=0, sort=False)\n",
    "    concat_dummies = pd.get_dummies(concat)\n",
    "    concat_dummies = pipeline.fit_transform(concat_dummies)\n",
    "    train_dummies = copy(concat_dummies[:n_train])\n",
    "    test_dummies = copy(concat_dummies[n_train:])\n",
    "    return train_dummies, test_dummies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape without missing rows = (30451, 49)\n",
      "class count:\n",
      " 0.0    29949\n",
      "1.0      502\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train, test = read_train_test()\n",
    "train = train.dropna(axis=0, how='all') # remove missing rows\n",
    "y_train = train.Class\n",
    "train = train.drop(columns='Class')\n",
    "\n",
    "columns_to_drop = ['Pre_L_Pupil_Day', 'T_L_Actual_AblDepth']\n",
    "train, test = remove_columns(train, test, columns_to_drop)\n",
    "\n",
    "print(\"train.shape without missing rows =\", train.shape)\n",
    "print(\"class count:\\n\", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### add here new steps to preprocess\n",
    "preprocessing = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean'))\n",
    "                ])\n",
    "X_train, X_test = preprocess_data(train, test, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'smote', 'rf', 'smote__k_neighbors', 'smote__kind', 'smote__m_neighbors', 'smote__n_jobs', 'smote__out_step', 'smote__random_state', 'smote__ratio', 'smote__sampling_strategy', 'smote__svm_estimator', 'rf__bootstrap', 'rf__class_weight', 'rf__criterion', 'rf__max_depth', 'rf__max_features', 'rf__max_leaf_nodes', 'rf__min_impurity_decrease', 'rf__min_impurity_split', 'rf__min_samples_leaf', 'rf__min_samples_split', 'rf__min_weight_fraction_leaf', 'rf__n_estimators', 'rf__n_jobs', 'rf__oob_score', 'rf__random_state', 'rf__verbose', 'rf__warm_start'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### add here new steps before predict (TODO: undersample)\n",
    "evaluation =  Pipeline([\n",
    "                ('smote', SMOTE(random_state=42, ratio=0.8, n_jobs=-1)),\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, n_jobs=-1))\n",
    "                ])\n",
    "evaluation.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n",
       "          error_score='raise-deprecating',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('smote', SMOTE(k_neighbors=5, kind='deprecated', m_neighbors='deprecated', n_jobs=-1,\n",
       "   out_step='deprecated', random_state=42, ratio=0.8,\n",
       "   sampling_strategy='auto', svm_estimator='deprecated')), ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "          ..._jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'rf__n_estimators': array([ 50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160, 170,\n",
       "       180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,\n",
       "       310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430,\n",
       "       440, 450, 460, 470, 480, 490]), 'rf__max_features': ['auto', 'sqrt'], 'rf__max_depth': [5, 10, 15, 20, 25, None], 'rf__min_samples_split': [2, 5, 10], 'smote__k_neighbors': array([2, 3, 4, 5, 6, 7, 8, 9])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Cross Validation\n",
    "\n",
    "n_splits = 10 # number of folds (stratified KFold)\n",
    "n_iter = 100 # number of parameter settings that are sampled\n",
    "\n",
    "### add here new hyperparameters\n",
    "rf_hyperparams = {\n",
    "    'rf__n_estimators': np.arange(start=50, stop=500, step=10),\n",
    "    'rf__max_features': ['auto', 'sqrt'],\n",
    "    'rf__max_depth': list(np.arange(5,30,5)) + [None],\n",
    "    'rf__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "smote_hyperparams = {\n",
    "    'smote__k_neighbors': np.arange(2,10)\n",
    "    # 'smote__ratio': ??? TODO\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(evaluation, \n",
    "                        param_distributions={**rf_hyperparams, **smote_hyperparams},\n",
    "                        scoring='roc_auc',\n",
    "                        n_iter=n_iter, \n",
    "                        cv=StratifiedKFold(n_splits=n_splits),\n",
    "                        n_jobs=-1)\n",
    "\n",
    "rs.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:\n",
      "\t{'smote__k_neighbors': 3, 'rf__n_estimators': 390, 'rf__min_samples_split': 2, 'rf__max_features': 'sqrt', 'rf__max_depth': 5}\n",
      "best auc score:\n",
      "\t0.5961\n"
     ]
    }
   ],
   "source": [
    "print('best params:\\n\\t{}'.format(rs.best_params_))\n",
    "print('best auc score:\\n\\t{:.4f}'.format(rs.best_score_))\n",
    "pred = rs.predict_proba(X_test)[:,1]\n",
    "\n",
    "ids = np.arange(1, pred.shape[0]+1)\n",
    "rf_df = pd.DataFrame({\"Id\" : ids, \"Class\" : pred})\n",
    "rf_df.to_csv(\"out/{}.csv\".format(\"rf_with_tune\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
